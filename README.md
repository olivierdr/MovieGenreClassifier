# Movie Genre Classifier

This project focuses on classifying movie synopses into one of three genres: cult, paranormal, or dramatic. It includes dataset exploration, cleaning, training and evaluation of two classification models, and a demo application built with Streamlit. A Dockerfile is also provided for easy containerized deployment.

## Dataset Exploration and Cleaning

As a first step, I explored the dataset and noticed that although the data was relatively well structured, several inconsistencies were present. Some rows featured the same movie title but with different synopses, others had identical synopses associated with different titles, and in some cases, the same title and synopsis were assigned different labels.

To address these issues, I designed the following data cleaning strategy:

- If a movie title appeared multiple times with different synopses, I computed their semantic similarity.  
  - If the synopses were not similar (similarity < 0.8), I flagged those entries as `ambiguous=True`.  
  - Otherwise, I kept the longest version.  

- If a synopsis was linked to multiple movie titles:  
  - If the labels matched, I kept the first instance (the title itself is not informative for classification).  
  - If the labels differed, the entry was flagged as `ambiguous=True`.

### Dataset Insights

- Original dataset size: 1,566 rows  
- Cleaned dataset size: 1,528 rows  
- Ambiguous entries flagged: 38 rows  
- No missing values found  
- 33 rows with duplicate titles  
- 40 rows with duplicate synopses  
- 22 pairs of similar synopses (similarity threshold: 0.8)  

**Label distribution:**  
- cult: 1,033 (65.96%)  
- paranormal: 366 (23.37%)  
- dramatic: 167 (10.66%)  

**Synopsis length statistics:**  
- Mean synopsis length: 5,219 characters  
- Strong concentration around 4,500 characters  
- Long tail of distribution reaching up to 50,000 characters  

## Modeling Approaches

Two distinct modeling strategies were employed to classify the synopses into the three genres.

### TF-IDF + Logistic Regression

The first approach uses a bag-of-words representation with TF-IDF vectorization followed by logistic regression to identify class-specific word usage patterns.

**Performance metrics:**  
- Macro-average F1-score: 58%  
- Weighted-average F1-score: 69%  

These metrics were chosen because the macro-average F1 treats all classes equally, which is important in the context of class imbalance, while the weighted-average F1 accounts for the support of each class.

### SentenceTransformer Embeddings + Classifier

The second approach uses embeddings generated by `SentenceTransformer('all-MiniLM-L6-v2')` to represent synopses as dense vectors. A classifier was then trained on these embeddings.

**Performance after 10 epochs:**  
- Macro-average F1-score: 32%  
- Weighted-average F1-score: 51%  

The embedding-based model mostly predicts the `cult` class and struggles to distinguish the others effectively.This suggests that word choice may be more discriminative than semantic similarity in this case. 

> **It also raises questions about the quality of the labeling process: Were the tags assigned manually or automatically? What were the criteria behind each class label? What do "cult", "paranormal", and "dramatic" concretely represent? Improving label consistency and clearly defining each class could significantly enhance classification performance.**


## Examples Highlighting Classification Challenges

These examples illustrate the subjectivity and ambiguity in labeling that can hinder model performance:

**"Fright Night" (1985)** – Tagged as **cult**, but has strong **paranormal** elements:
- The synopsis is heavily focused on vampire lore, supernatural events, and paranormal activity.
- Example: *"Charley suspects that Jerry Dandridge is a vampire, he seeks advice from Evil Ed about how to fight vampires... vampires are super strong, have no heartbeat... holy water and crosses."*
- Despite being labeled as "cult", the content clearly leans toward paranormal/horror, which could confuse a classifier.

**"The Sand Pebbles"** – Tagged as **cult**, but has strong **dramatic** themes:
- Focuses on moral dilemmas, political commentary, and character development.
- Example: *"For troubled hero Jake Holman (Steve 'Lightning' McQueen), no good deed goes unpunished. Po-Han is later captured and tortured by a Communist mob because he works for the Americans."*
- The "cult" label may refer to its lasting popularity rather than its content, which is highly dramatic in tone and theme.

These examples show how genre classification is not always tied directly to content but may depend on historical reception or audience interpretation, which is difficult for models to learn from synopses alone.

## Additional Analyses

It would be valuable to investigate which words are most discriminative for each class in the TF-IDF model and to visualize the embeddings in 2D or 3D space to better understand class separability. This can provide further insights into model behavior.

## Streamlit Application

A demo application was developed using Streamlit due to its simplicity and tight integration with Python. The app allows users to paste a synopsis and receive predictions from both models for comparison.

## Bonus Question – Multi-Label Classification

In cases where a movie can have multiple genre tags, possible approaches include one-vs-rest binary classifiers for each label, classifier chains to capture label dependencies, or neural networks with sigmoid outputs for multi-label prediction.

Challenges in this setting include handling label imbalance, accounting for label correlations, choosing appropriate evaluation metrics (such as micro/macro F1), and tuning thresholds for each class.

---

## Project Structure

```
MovieGenreClassifier/
├── data/
│   ├── raw/           # Original, immutable data
│   │   ├── task.csv           # Original dataset task
│   │   └── bonus_task.csv     # Optional Multi-label classification task
│   └── processed/     # Cleaned and processed data
│       └── movies_cleaned.csv  # Cleaned dataset
├── outputs/
│   ├── analysis/      # Data analysis results and plots
│   ├── logs/         # Log files for data processing and model training
│   └── models/       # Trained models and model artifacts
│       ├── movie_tfidf_model.joblib  # TF-IDF model
│       └── embedding_model.pth       # Embedding model
├── src/
│   ├── data/         # Data processing scripts
│   │   └── clean_dataset.py  # Data cleaning pipeline
│   ├── models/       # Model training and evaluation
│   │   ├── model.py      # Model definitions
│   │   └── train.py      # Training pipeline
│   ├── utils/        # Utility functions
│   └── app.py        # Streamlit web application
├── tests/            # Test files
├── Dockerfile        # Docker configuration
├── requirements.txt  # Project dependencies
└── README.md        # Project documentation
```

## Running the Project

### Local Setup

1. Create a virtual environment:
```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Train the models:
```bash
python src/models/train.py
```

4. Launch the Streamlit app:
```bash
streamlit run src/app.py
```

### Docker Setup

1. Build the Docker image:
```bash
docker build -t movie-genre-classifier .
```

2. Run the container:
```bash
docker run -p 8501:8501 movie-genre-classifier
```

The application will be available at `http://localhost:8501`

Note: The Docker container includes all necessary dependencies and models. If you want to retrain the models inside the container, you can mount the data directory:

```bash
docker run -p 8501:8501 -v $(pwd)/data:/app/data movie-genre-classifier
```



